{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"../utils/\")\n",
    "%aimport utils\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Conv1D, Flatten\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "import os\n",
    "import random as rn\n",
    "\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "set_random_seed(seed)\n",
    "rn.seed(seed)\n",
    "\n",
    "# maximum number of cores\n",
    "n_cores = 20\n",
    "\n",
    "K.set_session(K.tf.Session(config=K.tf.ConfigProto(\n",
    "    intra_op_parallelism_threads=n_cores,\n",
    "    inter_op_parallelism_threads=n_cores\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_name = 'BRCA'\n",
    "X_c, y_c = utils.get_cancer_data(cancer_name)\n",
    "print(\"Cancer: {}\".format(cancer_name))\n",
    "print(\"\\t#samples: {}\".format(X_c.shape[0]))\n",
    "print(\"\\t#genes: {}\".format(X_c.shape[1]))\n",
    "print(\"\\t#TUMORS: {}\\t#NORMAL: {}\".format(y_c[y_c == utils.TUMOR].shape[0], y_c[y_c == utils.NORMAL].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_features(X):\n",
    "    return np.arange(X.shape[1]) # nothing happens\n",
    "\n",
    "def preprocess(X):\n",
    "    scaler = MinMaxScaler()\n",
    "    return utils.pre_process(X, get_filtered_features, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv_model(input_size):\n",
    "    global stride\n",
    "    print(stride)\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=5, kernel_size=(stride), input_shape=(input_size, 1), \n",
    "                     activation='relu', strides=stride))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=200, activation=\"relu\"))\n",
    "    model.add(Dense(units=50, activation=\"relu\"))\n",
    "    model.add(Dense(units=1, activation=\"sigmoid\", name='output'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_training_default_1(X, y, train, test, preprocess, validation_split, seed):\n",
    "    X_train, y_train = X[train], y[train]\n",
    "    X_test, y_test = X[test], y[test]\n",
    "\n",
    "    # get the validation set in a stratified fashion from the training set\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=validation_split,\n",
    "                                                      random_state=seed, stratify=y_train)\n",
    "    print(\"Train - Test\")\n",
    "    # preprocess training set and get features and scaler\n",
    "    X_train, scaler, sel_features = preprocess(X_train)\n",
    "    print(\"Training scaled\")\n",
    "    # transform testing set\n",
    "    X_test = scaler.fit_transform(X_test[:, sel_features])\n",
    "    print(\"Test scaled\")\n",
    "    # transform validation set\n",
    "    X_val = scaler.fit_transform(X_val[:, sel_features])\n",
    "    print(\"Val scaled\")\n",
    "    oversampler = RandomOverSampler(random_state=seed)\n",
    "    \n",
    "    # oversampling\n",
    "    X_train, y_train =oversampler.fit_sample(X_train, y_train)\n",
    "    print(\"Train - oversampled\")\n",
    "    X_val, y_val = oversampler.fit_sample(X_val, y_val)\n",
    "    print(\"Val - oversampled\")\n",
    "#     print(X_train.shape)\n",
    "#     print(X_val.shape)\n",
    "    \n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], -1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], -1)\n",
    "    X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], -1)\n",
    "    \n",
    "    print(\"Reshaped\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = np.load(\"/home/nanni/Data/TCGA/CIBB/ontological_distance_matrix.npy\")\n",
    "np.fill_diagonal(dm, np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = np.argsort(dm, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature disposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 4\n",
    "stride = n_neighbors + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_idxs = np.append(np.arange(neighbors.shape[0]).reshape(-1, 1), neighbors[:, :n_neighbors], axis=1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_c_conv = X_c[:, conv_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvscores_c, histories_c = utils.cross_validation(X=X_c_conv, y=y_c, \n",
    "                                                 preprocess=preprocess, \n",
    "                                                 seed=seed, \n",
    "                                                 data_preparation=split_training_default_1,\n",
    "                                                 create_model=create_conv_model, \n",
    "                                                 get_measures=utils.get_measures)\n",
    "cvscores_c.mean().to_frame().T.drop(\"split\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvscores_c.to_excel(\"./results/ontological/results.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (deeplearning)",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
